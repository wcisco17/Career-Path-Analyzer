{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# go through this first: https://www.tensorflow.org/text/tutorials/classify_text_with_bert#export_for_inference\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from helper import remove_unused_columns, transform_profession, preprocess_text\n",
    "import transformers\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "from tensorflow.python.keras import layers, models\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clean up your data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "   category_id        categories  \\\n0            0  Machine Learning   \n1            1  Machine Learning   \n2            2  Machine Learning   \n3            3  Machine Learning   \n4            4  Machine Learning   \n\n                                          profession  categories_encode  \\\n0  Director of Data Science, Machine Learning at ...                  0   \n1        Machine learning on Encrypted data Engineer                  0   \n2  Machine Learning Research Scientist - Deep Lea...                  0   \n3             Principal (Manager) R&D Data Scientist                  0   \n4  Vice President of Machine Learning, Merchandis...                  0   \n\n                               clean_text_profession  \n0  director data science machine learning walmart...  \n1           machine learning encrypted data engineer  \n2  machine learning research scientist deep learning  \n3                principal manager rd data scientist  \n4  vice president machine learning merchandising ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>category_id</th>\n      <th>categories</th>\n      <th>profession</th>\n      <th>categories_encode</th>\n      <th>clean_text_profession</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Machine Learning</td>\n      <td>Director of Data Science, Machine Learning at ...</td>\n      <td>0</td>\n      <td>director data science machine learning walmart...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Machine Learning</td>\n      <td>Machine learning on Encrypted data Engineer</td>\n      <td>0</td>\n      <td>machine learning encrypted data engineer</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Machine Learning</td>\n      <td>Machine Learning Research Scientist - Deep Lea...</td>\n      <td>0</td>\n      <td>machine learning research scientist deep learning</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Machine Learning</td>\n      <td>Principal (Manager) R&amp;D Data Scientist</td>\n      <td>0</td>\n      <td>principal manager rd data scientist</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Machine Learning</td>\n      <td>Vice President of Machine Learning, Merchandis...</td>\n      <td>0</td>\n      <td>vice president machine learning merchandising ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading our files and\n",
    "file = '../excel-data/f-linkedin-profile.csv'\n",
    "data_csv = pd.read_csv(file)\n",
    "data_top = remove_unused_columns(data_csv)[0:13]\n",
    "\n",
    "# -------- Get our columns into lists --------\n",
    "\n",
    "headline_categories = list(data_csv['Industry'])\n",
    "\n",
    "\n",
    "def encode_labels(headline) -> ([int], dict):\n",
    "    categories_encode = {}\n",
    "    enc_values = []\n",
    "\n",
    "    for i, categories in enumerate(list(data_csv['Industry'])):\n",
    "        if categories not in categories_encode:\n",
    "            categories_encode[categories] = i\n",
    "\n",
    "    for item in headline:\n",
    "        num = categories_encode[item] if item in categories_encode else ''\n",
    "        enc_values.append(num)\n",
    "\n",
    "    return enc_values, categories_encode\n",
    "\n",
    "\n",
    "profession = transform_profession(data_top, data=data_csv)\n",
    "category_list = data_csv['Headline']\n",
    "category_id = [i for i in range(len(category_list))]\n",
    "encoded_labels, categories_label = encode_labels(headline_categories)\n",
    "\n",
    "# Initialize our columns into a dataframe\n",
    "dtf = pd.DataFrame()\n",
    "dtf['category_id'] = category_id\n",
    "dtf['categories'] = headline_categories\n",
    "dtf['profession'] = profession\n",
    "dtf['categories_encode'] = encoded_labels\n",
    "\n",
    "# Clean your data set first remove unwanted words like: \"I\", \"me\", \"you\"\n",
    "list_stop_of_words = stopwords.words('english')\n",
    "dtf['clean_text_profession'] = dtf['profession'].apply(\n",
    "    lambda x: preprocess_text(x, flg_stemm=False, flg_lemm=True, lst_stopwords=list_stop_of_words))\n",
    "\n",
    "dtf.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split data set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "## split dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(dtf['clean_text_profession'], dtf['categories_encode'], test_size=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using BERT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "nlp = transformers.TFBertModel.from_pretrained('bert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [26]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m encoded_data_train \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpad_to_max_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m256\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\n\u001B[1;32m      8\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m encoded_data_val \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mencode_plus(\n\u001B[1;32m     11\u001B[0m     X_val,\n\u001B[1;32m     12\u001B[0m     add_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     16\u001B[0m     return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     17\u001B[0m )\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils.py:785\u001B[0m, in \u001B[0;36mPreTrainedTokenizer.encode_plus\u001B[0;34m(self, text, text_pair, add_special_tokens, max_length, stride, truncation_strategy, return_tensors, **kwargs)\u001B[0m\n\u001B[1;32m    782\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    783\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 785\u001B[0m first_ids \u001B[38;5;241m=\u001B[39m \u001B[43mget_input_ids\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    786\u001B[0m second_ids \u001B[38;5;241m=\u001B[39m get_input_ids(text_pair) \u001B[38;5;28;01mif\u001B[39;00m text_pair \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    788\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_for_model(first_ids,\n\u001B[1;32m    789\u001B[0m                               pair_ids\u001B[38;5;241m=\u001B[39msecond_ids,\n\u001B[1;32m    790\u001B[0m                               max_length\u001B[38;5;241m=\u001B[39mmax_length,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    793\u001B[0m                               truncation_strategy\u001B[38;5;241m=\u001B[39mtruncation_strategy,\n\u001B[1;32m    794\u001B[0m                               return_tensors\u001B[38;5;241m=\u001B[39mreturn_tensors)\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils.py:783\u001B[0m, in \u001B[0;36mPreTrainedTokenizer.encode_plus.<locals>.get_input_ids\u001B[0;34m(text)\u001B[0m\n\u001B[1;32m    781\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m text\n\u001B[1;32m    782\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 783\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
     ]
    }
   ],
   "source": [
    "encoded_data_train = tokenizer.encode_plus(\n",
    "    X_train,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    pad_to_max_length=True,\n",
    "    max_length=256,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.encode_plus(\n",
    "    X_val,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    pad_to_max_length=True,\n",
    "    max_length=256,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# input_ids_train = encoded_data_train['input_ids']\n",
    "# attention_masks_train = encoded_data_train['attention_mask']\n",
    "# labels_train = torch.tensor(X_train)\n",
    "#\n",
    "# input_ids_val = encoded_data_val['input_ids']\n",
    "# attention_masks_val = encoded_data_val['attention_mask']\n",
    "# labels_val = torch.tensor(X_val)\n",
    "#\n",
    "#\n",
    "# dataset_train = torch.Tensor(input_ids_train, attention_masks_train, labels_train)\n",
    "# dataset_val = torch.Tensor(input_ids_val, attention_masks_val, labels_val)\n",
    "#\n",
    "# print(len(dataset_train), len(dataset_val))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compiling the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corpus = dtf['clean_text_profession']\n",
    "maxlen = 50\n",
    "\n",
    "maxqnans = np.int((maxlen - 20) / 2)\n",
    "corpus_tokenized = [\"[CLS] \" +\n",
    "                    \" \".join(\n",
    "                        tokenizer.tokenize(re.sub(r'[^\\w\\s]+|\\n', '', str(txt).lower().strip()))[:maxqnans]) + \" [SEP] \"\n",
    "                    for txt in corpus]\n",
    "\n",
    "## generate masks\n",
    "masks = [[1] * len(txt.split(\" \")) + [0] * (maxlen - len(\n",
    "    txt.split(\" \"))) for txt in corpus_tokenized]\n",
    "\n",
    "## padding\n",
    "txt2seq = [txt + \" [PAD]\" * (maxlen - len(txt.split(\" \"))) if len(txt.split(\" \")) != maxlen else txt for txt in\n",
    "           corpus_tokenized]\n",
    "\n",
    "## generate idx\n",
    "idx = [tokenizer.encode(seq.split(\" \")) for seq in txt2seq]\n",
    "\n",
    "## generate segments\n",
    "segments = []\n",
    "for seq in txt2seq:\n",
    "    temp, i = [], 0\n",
    "    for token in seq.split(\" \"):\n",
    "        temp.append(i)\n",
    "        if token == \"[SEP]\":\n",
    "            i += 1\n",
    "    segments.append(temp)\n",
    "## feature matrix\n",
    "X_train = [np.asarray(idx, dtype='int32'),\n",
    "           np.asarray(masks, dtype='int32'),\n",
    "           np.asarray(segments, dtype='int32')]\n",
    "\n",
    "i = 0\n",
    "print(\"txt: \", dtf['clean_text_profession'].iloc[0])\n",
    "print(\"tokenized:\", [tokenizer.convert_ids_to_tokens(idx) for idx in X_train[0][i].tolist()])\n",
    "print(\"idx: \", X_train[0][i])\n",
    "print(\"mask: \", X_train[1][i])\n",
    "print(\"segment: \", X_train[2][i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## inputs\n",
    "idx = layers.Input(50, dtype=\"int32\", name=\"input_idx\")\n",
    "masks = layers.Input(50, dtype=\"int32\", name=\"input_masks\")\n",
    "\n",
    "## pre-trained bert with config\n",
    "config = transformers.DistilBertConfig(dropout=0.2,\n",
    "                                       attention_dropout=0.2)\n",
    "config.output_hidden_states = False\n",
    "nlp = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
    "\n",
    "bert_out = nlp(idx, attention_mask=masks)[0]\n",
    "\n",
    "## fine-tuning\n",
    "x = layers.GlobalAveragePooling1D()(bert_out)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "y_out = layers.Dense(len(np.unique(y_train)), activation='softmax')(x)\n",
    "\n",
    "## compile\n",
    "model = models.Model([idx, masks], y_out)\n",
    "\n",
    "for layer in model.layers[:3]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## encode y\n",
    "X_test = dtf_test['clean_text_profession']\n",
    "\n",
    "dic_y_mapping = {n: label for n, label in enumerate(np.unique(y_train))}\n",
    "inverse_dic = {v: k for k, v in dic_y_mapping.items()}\n",
    "\n",
    "y_train = np.array([inverse_dic[y] for y in y_train])\n",
    "\n",
    "# ## train\n",
    "training = model.fit(x=X_train, y=y_train, batch_size=64, epochs=1, shuffle=True, verbose=1)\n",
    "\n",
    "## test\n",
    "predicted_prob = model.predict(X_test)\n",
    "predicted = [dic_y_mapping[np.argmax(pred)] for pred in predicted_prob]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}